{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ratulesrar/anaconda3/lib/python3.5/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/Users/ratulesrar/anaconda3/lib/python3.5/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import statsmodels.formula.api as smf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.grid_search import ParameterGrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "df = pd.read_pickle('data/clean_speech_approvals.pkl')\n",
    "df.set_index('president')\n",
    "\n",
    "feat = ['approval', 'compound', 'positive', 'negative',\n",
    "        'neutral', 'year', 'party', 'approve_mean', 'approve_median',\n",
    "        'approve_min', 'approve_max', 'approve_std', 'disapprove_mean',\n",
    "        'disapprove_median', 'disapprove_min', 'disapprove_max',\n",
    "        'disapprove_std', 'num_words', 'num_sentences']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_approval(approve):\n",
    "    if approve > 50:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def to_binary(df, col, val='t'):\n",
    "    '''\n",
    "    Converts column to binary based on value\n",
    "    '''\n",
    "    df[col] = df[col].apply(lambda x: 1 if x == val else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_binary(df, 'party', 'D')\n",
    "\n",
    "df['approval'] = df['approve_mean'].apply(binary_approval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create X, y vectors\n",
    "X = df.filter(feat)\n",
    "X = X.drop('approval', axis=1)\n",
    "y = df['approval'].to_frame()\n",
    "\n",
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=3)\n",
    "\n",
    "# validation split\n",
    "X_test, X_validation, y_test, y_validation = train_test_split(X_test, y_test, test_size=0.5, random_state=3)\n",
    "\n",
    "# set model parameters\n",
    "params = {'penalty': ['l1','l2'], 'C': [0.001,0.1,1,10]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation functions\n",
    "# calculate precision, recall and auc metrics\n",
    "\n",
    "def plot_roc(name, probs, true, output_type):\n",
    "    fpr, tpr, thresholds = roc_curve(true, probs)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    pl.clf()\n",
    "    pl.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "    pl.plot([0, 1], [0, 1], 'k--')\n",
    "    pl.xlim([0.0, 1.05])\n",
    "    pl.ylim([0.0, 1.05])\n",
    "    pl.xlabel('False Positive Rate')\n",
    "    pl.ylabel('True Positive Rate')\n",
    "    pl.title(name)\n",
    "    pl.legend(loc=\"lower right\")\n",
    "    if (output_type == 'save'):\n",
    "        plt.savefig(name)\n",
    "    elif (output_type == 'show'):\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "def generate_binary_at_k(y_scores, k):\n",
    "    cutoff_index = int(len(y_scores) * (k / 100.0))\n",
    "    predictions_binary = [1 if x < cutoff_index else 0 for x in range(len(y_scores))]\n",
    "    return predictions_binary\n",
    "\n",
    "def precision_at_k(y_true, y_scores, k):\n",
    "    #y_scores_sorted, y_true_sorted = zip(*sorted(zip(y_scores, y_true), reverse=True))\n",
    "    y_scores_sorted, y_true_sorted = joint_sort_descending(np.array(y_scores), np.array(y_true))\n",
    "    preds_at_k = generate_binary_at_k(y_scores_sorted, k)\n",
    "    #precision, _, _, _ = metrics.precision_recall_fscore_support(y_true, preds_at_k)\n",
    "    #precision = precision[1]  # only interested in precision for label 1\n",
    "    precision = precision_score(y_true_sorted, preds_at_k)\n",
    "    return precision\n",
    "\n",
    "def recall_at_k(y_true, y_scores, k):\n",
    "    #y_scores_sorted, y_true_sorted = zip(*sorted(zip(y_scores, y_true), reverse=True))\n",
    "    y_scores_sorted, y_true_sorted = joint_sort_descending(np.array(y_scores), np.array(y_true))\n",
    "    preds_at_k = generate_binary_at_k(y_scores_sorted, k)\n",
    "    #precision, _, _, _ = metrics.precision_recall_fscore_support(y_true, preds_at_k)\n",
    "    #precision = precision[1]  # only interested in precision for label 1\n",
    "    recall = recall_score(y_true_sorted, preds_at_k)\n",
    "    return recall\n",
    "\n",
    "def f1_at_k(y_true, y_scores, k):\n",
    "    preds_at_k = generate_binary_at_k(y_scores, k)\n",
    "    return f1_score(y_true, preds_at_k, average='binary')\n",
    "\n",
    "def get_feature_importance(clf, model_name):\n",
    "    clfs = {'RF':'feature_importances',\n",
    "            'LR': 'coef',\n",
    "            'SVM': 'coef',\n",
    "            'DT': 'feature_importances',\n",
    "            'KNN': None,\n",
    "            'AB': 'feature_importances',\n",
    "            'GB': None,\n",
    "            'linear.SVC': 'coef',\n",
    "            'ET': 'feature_importances'\n",
    "            }\n",
    "\n",
    "    if clfs[model_name] == 'feature_importances':\n",
    "        return  list(clf.feature_importances_)\n",
    "    elif clfs[model_name] == 'coef':\n",
    "        return  list(clf.coef_.tolist())\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def plot_precision_recall_n(y_true, y_prob, model_name, output_type):\n",
    "    from sklearn.metrics import precision_recall_curve\n",
    "    y_score = y_prob\n",
    "    precision_curve, recall_curve, pr_thresholds = precision_recall_curve(y_true, y_score)\n",
    "    precision_curve = precision_curve[:-1]\n",
    "    recall_curve = recall_curve[:-1]\n",
    "    pct_above_per_thresh = []\n",
    "    number_scored = len(y_score)\n",
    "    for value in pr_thresholds:\n",
    "        num_above_thresh = len(y_score[y_score>=value])\n",
    "        pct_above_thresh = num_above_thresh / float(number_scored)\n",
    "        pct_above_per_thresh.append(pct_above_thresh)\n",
    "    pct_above_per_thresh = np.array(pct_above_per_thresh)\n",
    "\n",
    "    plt.clf()\n",
    "    fig, ax1 = plt.subplots()\n",
    "    ax1.plot(pct_above_per_thresh, precision_curve, 'b')\n",
    "    ax1.set_xlabel('percent of population')\n",
    "    ax1.set_ylabel('precision', color='b')\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(pct_above_per_thresh, recall_curve, 'r')\n",
    "    ax2.set_ylabel('recall', color='r')\n",
    "    ax1.set_ylim([0,1])\n",
    "    ax1.set_ylim([0,1])\n",
    "    ax2.set_xlim([0,1])\n",
    "\n",
    "    name = model_name\n",
    "    plt.title(name)\n",
    "    if (output_type == 'save'):\n",
    "        plt.savefig(name)\n",
    "    elif (output_type == 'show'):\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clf_loop(X_train, X_test, y_train, y_test, params):\n",
    "    '''\n",
    "    Function to loop through hyperparameters in a model\n",
    "    '''\n",
    "    results_df = pd.DataFrame(columns=('clf','parameters','auc-roc','accuracy'))\n",
    "    for p in ParameterGrid(params):\n",
    "        try:\n",
    "            clf = LogisticRegression()\n",
    "            clf.set_params(**p)\n",
    "            clf.fit(X_train, y_train)\n",
    "            y_pred = clf.predict(X_test)\n",
    "            y_pred_probs = list(clf.predict_proba(X_test)[:,1])\n",
    "            results_df.loc[len(results_df)] = [clf,p,roc_auc_score(y_test,y_pred_probs),\n",
    "                                               accuracy_score(y_test,y_pred)]\n",
    "            #plot_precision_recall_n(y_test, y_pred_probs, 'LR', 'show')\n",
    "        except IndexError:\n",
    "            print('IndexError')\n",
    "            continue\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.07987864, 0.55178373, 0.55328869, 0.32227806, 0.73842045,\n",
       "       0.83837443, 0.20360718, 0.26207714, 0.57778655, 0.25874635,\n",
       "       0.35862067, 0.95566582, 0.41597738])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run methods loop\n",
    "df_validation = clf_loop(X_train, X_test, y_train, y_test, params)\n",
    "\n",
    "# obtain params for model with max auc-roc\n",
    "best_model_params = dict(df_validation.sort_values('auc-roc', ascending=False)[:1]['parameters'])\n",
    "key = list(best_model_params.keys())[0]\n",
    "\n",
    "# evaluate using test data\n",
    "lr = LogisticRegression()\n",
    "lr.set_params(**best_model_params[key])\n",
    "lr.fit(X_validation, y_validation)\n",
    "\n",
    "# get predicted scores for test set\n",
    "y_pred = lr.predict_proba(X_test)[:, 1]\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
